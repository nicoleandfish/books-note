# ch8 自然语言处理（Natural Language Processing,NLP）
## 8.1 Jieba分词基础
### 8.1.1 Jieba中文分词
中文分词主要分为：正向最大匹配法、反向最大匹配法、分词与词性标注一体化方法、最佳匹配法、专家系统方法、最少分词词频选择方法和神经网络方法等。
Jieba分词主要设计的算法介绍：
1.基于前缀辞典实现高效的词图扫描，生成句子中汉字所有可能的成词情况所构成的有向无环图（Directed Acyclic Graph,DAG）
2.采用了动态规划查找最大概率路径，找出基于词频的最大切分组合。
3.对于未登陆词，采用了基于汉字成词能力的HMM模型，使用Viterbi算法。
现在开源的中文粉刺工具有：SnowNLP、THULAC、Jieba和HanLP。
### 8.1.2 Jieba分词的3种模式
精准模式、全模式和搜索引擎模式。
