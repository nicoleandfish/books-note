# Ch2 感知机
感知机（perceptron）是由美国学者Frank Rosenblatt在1957年提出来的。

## 2.1 感知机是什么
感知机接受多个输入信号，输出一个信号。
‘’‘mermaid
graph LR
A(x1) -- w1 --> B((圆))
C(x2)--w2-->B
’‘’
用公式表示为：
$
y
\begin{cases}
\ 0,    (w_1x_1+w_2x_2 \leqslant \theta) \\
 1, (w_1x_1+w_2x_2 > \theta) 
\end{cases}
$


## 2.2 简单逻辑电路
### 2.2.1 与门（AND gate）
与门真值表
x1| x2     | y
----|---- | -----
0|0|0
0|1|0
1|0|0
1|1|1

其参数选择不唯一。例如$(w_1,w_2,\theta)=(0.5,0.5,0.7)$，或（0.5,0.5,0.8）或者（1.0,1.0,1.0）时同样满足条件。
### 与或门（NAND gate）和非门（OR gate）
与或门真值表
x1| x2     | y
----|---- | -----
0|0|1
0|1|1
1|0|1
1|1|0
$(w_1,w_2,\theta)=(-0.5,-0.5,-0.7)$可以满足条件。

或门真值表
x1| x2     | y
----|---- | -----
0|0|0
0|1|1
1|0|1
1|1|1

$(w_1,w_2,\theta)=(1,1,0.9)$可以满足条件。

## 2.3 感知机的实现
### 2.3.1 与门AND函数简单实现：

```javascript
def AND(x1,x2):
	w1,w2,theta=0.5,0.5,0.7
	tmp=x1*w1+x2*w2
	if tmp <= theta:
		return 0
	elif tmp > theta:	
		return 1
```

### 2.3.2 导入权重和偏置

### 2.3.3 

# Ch4 神经网络的学习
所谓学习，是指从训练数据中自动获取最优权重参数的过程。
## 4.1 从数据中学习
神经网络的特征就是可以才能够数据中学习，即可以由数据自动决定权重参数的值。
### 4.1.1 数据驱动
如何从零开始想出一个可识别手写5的算法，可以考虑利用数据来解决这个问题（手写识别）。
一种为，先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。特征量为从输入数据（包括图像）中准确低提取本质数据（重要的数据）的转换器。图像的特征量通常表示用向量表示。在计算机领域，常用的特征量包括SIFT、SURF和HOG等。使用这些特征量将图像数据转换成向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类起进行学习。不同的问题，采用不同的特征量。
第二种为，神经网络（深度学习）的方法，此方法没有人工介入，即没有人为设计和选取特征量。
### 4.1.2 训练数据和测试数据
机器学习中，数据分为训练数据和测试数据。分成两类数据，是因为对模型的泛化能力有要求。测试数据不允许被包括在训练数据中，训练数据也可被称为监督数据。
泛化能力是指处理未被观察果的数据（不包含在训练数据中的数据）的能力。类似与模拟考和真实考。获得泛化能力是机器学习的最终目标。
只对某个数据集过度拟合的状态称为过拟合（over fitting）。

## 4.2 损失函数
神经网络以某个指标为线索寻找最优权重参数，所用的指标称为"损失函数"（loss function）。损失函数可自行定义，一般使用均方误差和交叉熵误差。

### 4.2.1 均方误差（mean squared error-MSE）
E=1
```
def mean_squared_error(y,t):
	return 0.5 * np.sum((y-t) ** 2)
```

### 4.2.2 交叉熵误差（cross entropy error-CEE）

```
def cross_entropy_error(y,t):
	delta = 1e-7
	return -np.sum(t * np.log(y+delta))
```

### 4.2.3 mini-batch学习

# ch6 与学习相关的技巧
## 6.1参数的更新
神经网络的学习的目的是找到损失函数的值尽可能小的参数。这个过程称为最优化（optimization）。
为了找到最优参数，将参数的梯度（导数）作为依据，沿梯度方向更新参数，并重复此步骤，从而找到一定精度下的最优参数。这个过程为随机梯度下降法（stochastic gradient descent），简称SGD。
###6.1.1 SGD
···
class SGD:
	def __init__(self,lr=0.01):
		self.lr=lr
	def update(self, params, grads):
		for key in params.keys():
			params[key] -= self.lr * grads[key]
···

# Ch8 深度学习
深度学习是加深了层的深度神经网络。


## 8.1 加深网络

